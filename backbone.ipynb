{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class VideoLandmarkDataset(Dataset):\n",
    "    def __init__(self, anno_path, video_prefix, landmark_prefix, mode, clip_len, frame_sample_rate, crop_size, short_side_size, args):\n",
    "        \"\"\"\n",
    "        Initialize the VideoLandmarkDataset.\n",
    "        \n",
    "        Args:\n",
    "            anno_path (str): Path to the annotation CSV file.\n",
    "            video_prefix (str): Directory containing video files.\n",
    "            landmark_prefix (str): Directory containing landmark files.\n",
    "            mode (str): Dataset mode ('train', 'validation', 'test').\n",
    "            clip_len (int): Number of frames per video clip.\n",
    "            frame_sample_rate (int): Frame sampling rate.\n",
    "            crop_size (int): Crop size for frames.\n",
    "            short_side_size (int): Short side resizing for frames.\n",
    "            args (Namespace): Additional arguments.\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_sample_rate = frame_sample_rate\n",
    "        self.crop_size = crop_size\n",
    "        self.short_side_size = short_side_size\n",
    "\n",
    "        self.video_prefix = video_prefix\n",
    "        self.landmark_prefix = landmark_prefix\n",
    "\n",
    "        # Load annotations\n",
    "        self.annotations = self._load_annotations(anno_path)\n",
    "\n",
    "    def _load_annotations(self, anno_path):\n",
    "        \"\"\"\n",
    "        Load annotation data from a CSV file.\n",
    "        \"\"\"\n",
    "        annotations = []\n",
    "        with open(anno_path, 'r') as file:\n",
    "            for line in file:\n",
    "                video_file, landmark_file = line.strip().split(',')\n",
    "                video_path = os.path.join(self.video_prefix, video_file)\n",
    "                landmark_path = os.path.join(self.landmark_prefix, landmark_file)\n",
    "\n",
    "                if not os.path.exists(video_path):\n",
    "                    raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "                if not os.path.exists(landmark_path):\n",
    "                    raise FileNotFoundError(f\"Landmark file not found: {landmark_path}\")\n",
    "\n",
    "                annotations.append((video_path, landmark_path))\n",
    "        return annotations\n",
    "\n",
    "    def _load_video(self, video_path):\n",
    "        \"\"\"\n",
    "        Load video frames and preprocess them.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        success, frame = cap.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "            if count % self.frame_sample_rate == 0:\n",
    "                # Resize frame\n",
    "                frame_resized = cv2.resize(frame, (self.crop_size, self.crop_size))\n",
    "                frames.append(frame_resized)\n",
    "            success, frame = cap.read()\n",
    "            count += 1\n",
    "        cap.release()\n",
    "\n",
    "        # Limit to clip length and preprocess\n",
    "        frames = np.array(frames[:self.clip_len])\n",
    "        if len(frames) < self.clip_len:\n",
    "            raise ValueError(f\"Insufficient frames in video: {video_path}\")\n",
    "        frames = frames.transpose(0, 3, 1, 2)  # Convert to (T, C, H, W)\n",
    "        return torch.tensor(frames, dtype=torch.float32) / 255.0\n",
    "\n",
    "    def _load_landmarks(self, landmark_path):\n",
    "        \"\"\"\n",
    "        Load landmark data and preprocess it.\n",
    "        \"\"\"\n",
    "        landmarks = np.loadtxt(landmark_path, delimiter=',')  # CSV with x, y columns\n",
    "        landmarks = landmarks.reshape(self.clip_len, -1, 2)  # (T, num_landmarks, 2)\n",
    "        return torch.tensor(landmarks, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieve a sample from the dataset.\n",
    "        \"\"\"\n",
    "        video_path, landmark_path = self.annotations[index]\n",
    "        video = self._load_video(video_path)\n",
    "        landmarks = self._load_landmarks(landmark_path)\n",
    "        return video, landmarks\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(is_train, test_mode, args):\n",
    "    \"\"\"\n",
    "    Build a dataset for video input and target landmarks.\n",
    "    \n",
    "    Args:\n",
    "        is_train (bool): Whether the dataset is for training.\n",
    "        test_mode (bool): Whether the dataset is for testing.\n",
    "        args (Namespace): Configuration arguments containing dataset paths and parameters.\n",
    "\n",
    "    Returns:\n",
    "        dataset (Dataset): An instance of the dataset.\n",
    "        nb_classes (int or None): Number of classes (None for non-classification tasks).\n",
    "    \"\"\"\n",
    "    mode = 'train' if is_train else 'test' if test_mode else 'validation'\n",
    "    anno_file = os.path.join(args.data_path, f\"{mode}.csv\")\n",
    "    if not os.path.exists(anno_file):\n",
    "        raise FileNotFoundError(f\"Annotation file not found: {anno_file}\")\n",
    "\n",
    "    dataset = VideoLandmarkDataset(\n",
    "        anno_path=anno_file,\n",
    "        video_prefix=args.video_prefix,\n",
    "        landmark_prefix=args.landmark_prefix,\n",
    "        mode=mode,\n",
    "        clip_len=args.num_frames,\n",
    "        frame_sample_rate=args.sampling_rate,\n",
    "        crop_size=args.input_size,\n",
    "        short_side_size=args.short_side_size,\n",
    "        args=args,\n",
    "    )\n",
    "\n",
    "    nb_classes = None  # This is not a classification task\n",
    "    print(f\"Dataset built successfully for mode: {mode}\")\n",
    "    return dataset, nb_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        # Dataset parameters\n",
    "        self.data_set = 'UCF101'\n",
    "        self.data_path = 'your_data_path'\n",
    "        self.prefix = ''\n",
    "        self.split = ' '\n",
    "        self.filename_tmpl = 'img_{:05}.jpg'\n",
    "        self.nb_classes = 101\n",
    "        self.use_decord = True\n",
    "        self.trimmed = 60\n",
    "        self.time_stride = 16\n",
    "        \n",
    "        # Model parameters\n",
    "        self.input_size = 224\n",
    "        self.short_side_size = 224\n",
    "        self.num_frames = 16\n",
    "        self.sampling_rate = 4\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = 32\n",
    "        self.num_workers = 4\n",
    "        self.learning_rate = 0.001\n",
    "        self.log_interval = 10\n",
    "        self.epochs = 100\n",
    "        self.test_num_segment = 5\n",
    "        self.test_num_crop = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialLandmarkCNN(nn.Module):\n",
    "    def __init__(self, args: Parameters, num_landmarks: int = 68):\n",
    "        super(FacialLandmarkCNN, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        # Load and modify ResNet backbone\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        # Remove final FC and pooling layers\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        \n",
    "        # Calculate output size from backbone\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, args.input_size, args.input_size)\n",
    "            backbone_output = self.backbone(dummy_input)\n",
    "            self.backbone_output_shape = backbone_output.shape[1:]\n",
    "        \n",
    "        # Temporal modeling with 3D convolutions\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv3d(self.backbone_output_shape[0], 128, kernel_size=(3, 3, 3), \n",
    "                     padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to ensure consistent size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((56, 56))\n",
    "        \n",
    "        # Refinement network with proper size handling\n",
    "        self.refinement = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Landmark prediction head\n",
    "        self.landmark_head = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_landmarks * 2, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights using Kaiming initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Conv3d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, num_frames, c, h, w = x.shape\n",
    "        \n",
    "        # Validate input dimensions\n",
    "        if num_frames != self.args.num_frames:\n",
    "            raise ValueError(f\"Expected {self.args.num_frames} frames, got {num_frames}\")\n",
    "        if h != self.args.input_size or w != self.args.input_size:\n",
    "            raise ValueError(f\"Expected size {self.args.input_size}, got {h}x{w}\")\n",
    "        \n",
    "        # Process all frames in parallel\n",
    "        x = x.view(batch_size * num_frames, c, h, w)\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Apply adaptive pooling to ensure consistent size\n",
    "        features = self.adaptive_pool(features)\n",
    "        \n",
    "        # Extract landmarks and feature maps\n",
    "        landmarks = self.landmark_head(features)\n",
    "        landmarks = landmarks.view(batch_size, num_frames, -1, 2)\n",
    "        \n",
    "        # Generate feature maps\n",
    "        feature_maps = torch.sigmoid(self.refinement(features))\n",
    "        feature_maps = feature_maps.view(batch_size, num_frames, 1, h, w)\n",
    "        \n",
    "        return feature_maps, landmarks\n",
    "\n",
    "class FacialLandmarkDetector:\n",
    "    def __init__(self, args: Parameters):\n",
    "        self.args = args\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = FacialLandmarkCNN(args).to(self.device)\n",
    "        \n",
    "        # Initialize loss functions with reduction method\n",
    "        self.criterion_landmarks = nn.MSELoss(reduction='mean')\n",
    "        self.criterion_features = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        \n",
    "        # Initialize optimizer with weight decay\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.args.learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Initialize learning rate scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=5,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Initialize loss history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def create_landmark_map(self, landmarks: torch.Tensor, height: int, \n",
    "                          width: int, sigma: float = 3.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create Gaussian heatmaps for landmarks with numerical stability improvements.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: Tensor of shape (batch_size, num_landmarks, 2)\n",
    "            height: Height of output heatmap\n",
    "            width: Width of output heatmap\n",
    "            sigma: Standard deviation for Gaussian kernel\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, 1, height, width)\n",
    "        \"\"\"\n",
    "        batch_size = landmarks.size(0)\n",
    "        landmark_map = torch.zeros(batch_size, 1, height, width, \n",
    "                                 device=self.device)\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        y_grid, x_grid = torch.meshgrid(\n",
    "            torch.arange(height, device=self.device),\n",
    "            torch.arange(width, device=self.device)\n",
    "        )\n",
    "        \n",
    "        # Pre-compute maximum square distance for clipping\n",
    "        max_squared_dist = 9 * sigma * sigma\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for landmark in landmarks[b]:\n",
    "                x, y = landmark\n",
    "                \n",
    "                # Skip if landmark is outside image bounds\n",
    "                if not (0 <= x < width and 0 <= y < height):\n",
    "                    continue\n",
    "                \n",
    "                # Compute distances efficiently using broadcasting\n",
    "                squared_dist = (x_grid - x) ** 2 + (y_grid - y) ** 2\n",
    "                \n",
    "                # Clip distances for numerical stability\n",
    "                squared_dist = torch.clamp(squared_dist, max=max_squared_dist)\n",
    "                \n",
    "                # Compute Gaussian values\n",
    "                gaussian = torch.exp(-squared_dist / (2 * sigma * sigma))\n",
    "                \n",
    "                # Update landmark map using maximum values\n",
    "                landmark_map[b, 0] = torch.maximum(landmark_map[b, 0], gaussian)\n",
    "        \n",
    "        return landmark_map\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        \"\"\"Train for one epoch and return average loss.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        for batch_idx, (data, target_landmarks) in enumerate(train_loader):\n",
    "            # Skip invalid batches\n",
    "            if data.size(1) != self.args.num_frames:\n",
    "                continue\n",
    "            \n",
    "            # Move data to device\n",
    "            data = data.to(self.device)\n",
    "            target_landmarks = target_landmarks.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            feature_maps, predicted_landmarks = self.model(data)\n",
    "            \n",
    "            # Create target maps efficiently\n",
    "            target_maps = self.create_landmark_map(\n",
    "                target_landmarks.view(-1, target_landmarks.size(-2), 2),\n",
    "                data.size(-2), data.size(-1)\n",
    "            ).view_as(feature_maps)\n",
    "            \n",
    "            # Compute losses\n",
    "            landmark_loss = self.criterion_landmarks(\n",
    "                predicted_landmarks, target_landmarks\n",
    "            )\n",
    "            feature_loss = self.criterion_features(feature_maps, target_maps)\n",
    "            loss = landmark_loss + feature_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Log progress\n",
    "            if batch_idx % self.args.log_interval == 0:\n",
    "                logger.info(\n",
    "                    f'Train Batch: {batch_idx}/{num_batches} '\n",
    "                    f'Loss: {loss.item():.6f}'\n",
    "                )\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        \"\"\"Validate the model and return average validation loss.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(val_loader)\n",
    "        \n",
    "        for data, target_landmarks in val_loader:\n",
    "            if data.size(1) != self.args.num_frames:\n",
    "                continue\n",
    "            \n",
    "            data = data.to(self.device)\n",
    "            target_landmarks = target_landmarks.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            feature_maps, predicted_landmarks = self.model(data)\n",
    "            \n",
    "            # Create target maps\n",
    "            target_maps = self.create_landmark_map(\n",
    "                target_landmarks.view(-1, target_landmarks.size(-2), 2),\n",
    "                data.size(-2), data.size(-1)\n",
    "            ).view_as(feature_maps)\n",
    "            \n",
    "            # Compute losses\n",
    "            landmark_loss = self.criterion_landmarks(\n",
    "                predicted_landmarks, target_landmarks\n",
    "            )\n",
    "            feature_loss = self.criterion_features(feature_maps, target_maps)\n",
    "            loss = landmark_loss + feature_loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader, \n",
    "              num_epochs: Optional[int] = None) -> None:\n",
    "        \"\"\"Train the model for specified number of epochs.\"\"\"\n",
    "        num_epochs = num_epochs or self.args.epochs\n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            logger.info(f'\\nEpoch: {epoch+1}/{num_epochs}')\n",
    "            \n",
    "            # Train and validate\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.validate(val_loader)\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Store losses\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            logger.info(f'Training Loss: {train_loss:.6f}')\n",
    "            logger.info(f'Validation Loss: {val_loss:.6f}')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                \n",
    "                # Save checkpoint\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'train_losses': self.train_losses,\n",
    "                    'val_losses': self.val_losses\n",
    "                }, 'best_model.pth')\n",
    "                \n",
    "                logger.info('Saved best model checkpoint')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                logger.info(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "                break\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, video_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract features and landmarks from a video tensor.\"\"\"\n",
    "        self.model.eval()\n",
    "        video_tensor = video_tensor.to(self.device)\n",
    "        \n",
    "        if video_tensor.size(1) != self.args.num_frames:\n",
    "            raise ValueError(\n",
    "                f\"Expected {self.args.num_frames} frames, \"\n",
    "                f\"got {video_tensor.size(1)}\"\n",
    "            )\n",
    "        \n",
    "        feature_maps, landmarks = self.model(video_tensor)\n",
    "        return feature_maps, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize parameters\n",
    "    args = Parameters()\n",
    "    logger.info(\"Initialized parameters\")\n",
    "    \n",
    "    # Create detector instance\n",
    "    detector = FacialLandmarkDetector(args)\n",
    "    logger.info(\"Created facial landmark detector\")\n",
    "    \n",
    "    try:\n",
    "        train_dataset, _ = build_dataset(is_train=True, test_mode=False, args=args)\n",
    "        val_dataset, _ = build_dataset(is_train=False, test_mode=False, args=args)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=args.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers, \n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=args.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=args.num_workers, \n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Training configuration\n",
    "        logger.info(\"Starting training...\")\n",
    "        detector.train(train_loader, val_loader)\n",
    "        logger.info(\"Training completed successfully\")\n",
    "        \n",
    "        # Save final model\n",
    "        torch.save({\n",
    "            'model_state_dict': detector.model.state_dict(),\n",
    "            'optimizer_state_dict': detector.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': detector.scheduler.state_dict(),\n",
    "            'train_losses': detector.train_losses,\n",
    "            'val_losses': detector.val_losses\n",
    "        }, 'final_model.pth')\n",
    "        logger.info(\"Saved final model\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during training: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return detector\n",
    "\n",
    "def load_pretrained_detector(checkpoint_path: str, args: Parameters = None) -> FacialLandmarkDetector:\n",
    "    \"\"\"\n",
    "    Load a pretrained facial landmark detector from a checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "        args: Optional Parameters object. If None, default parameters will be used.\n",
    "        \n",
    "    Returns:\n",
    "        FacialLandmarkDetector: Loaded detector with pretrained weights\n",
    "    \"\"\"\n",
    "    if args is None:\n",
    "        args = Parameters()\n",
    "    \n",
    "    try:\n",
    "        # Create detector instance\n",
    "        detector = FacialLandmarkDetector(args)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=detector.device)\n",
    "        \n",
    "        # Load model state\n",
    "        detector.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Optionally load optimizer and scheduler states if they exist\n",
    "        if 'optimizer_state_dict' in checkpoint:\n",
    "            detector.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            detector.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        # Load training history if available\n",
    "        if 'train_losses' in checkpoint:\n",
    "            detector.train_losses = checkpoint['train_losses']\n",
    "        if 'val_losses' in checkpoint:\n",
    "            detector.val_losses = checkpoint['val_losses']\n",
    "        \n",
    "        logger.info(f\"Successfully loaded pretrained model from {checkpoint_path}\")\n",
    "        return detector\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading pretrained model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        detector = main()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Program terminated with error: {str(e)}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
