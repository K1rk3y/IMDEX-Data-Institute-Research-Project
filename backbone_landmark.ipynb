{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple, Optional\n",
    "import logging\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_existing_folders(directory, start, end):\n",
    "        existing_folders = []\n",
    "        for i in range(start, end + 1):\n",
    "            folder_name = f\"{i:03}\"  # Format number with leading zeros\n",
    "            folder_path = os.path.join(directory, folder_name)\n",
    "            if os.path.isdir(folder_path):  # Check if the folder exists\n",
    "                existing_folders.append(folder_name)\n",
    "        return existing_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VW300Dataset(Dataset):\n",
    "    def __init__(self, data_root, mode, clip_len=16, frame_sample_rate=2, crop_size=224, use_augmentation=False):\n",
    "        \"\"\"\n",
    "        Initialize the 300-VW dataset loader.\n",
    "        \n",
    "        Args:\n",
    "            data_root (str): Root directory containing the dataset folders\n",
    "            mode (str): Dataset split ('train', 'validation', 'test')\n",
    "            clip_len (int): Number of frames per video clip\n",
    "            frame_sample_rate (int): Frame sampling rate\n",
    "            crop_size (int): Size to crop/resize frames to\n",
    "            use_augmentation (bool): Whether to use data augmentation in training mode\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.mode = mode\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_sample_rate = frame_sample_rate\n",
    "        self.crop_size = crop_size\n",
    "        self.use_augmentation = use_augmentation\n",
    "        \n",
    "        # Load clip directories based on the split\n",
    "        self.clips = self._get_split_clips()\n",
    "        self.annotations = self._load_annotations()\n",
    "        \n",
    "    def _get_split_clips(self):\n",
    "        \"\"\"\n",
    "        Get list of clip directories based on the dataset split.\n",
    "        Implement according to 300-VW official split.\n",
    "        \"\"\"\n",
    "        # These splits should be adjusted according to official 300-VW protocol\n",
    "        splits = {\n",
    "            'train': generate_existing_folders(self.data_root, 1, 480),  # Add actual training clip numbers\n",
    "            'validation': generate_existing_folders(self.data_root, 481, 530),     # Add actual validation clip numbers\n",
    "            'test': generate_existing_folders(self.data_root, 531, 570)           # Add actual test clip numbers\n",
    "        }\n",
    "        print(splits)\n",
    "        return [os.path.join(self.data_root, clip_id) for clip_id in splits[self.mode]]\n",
    "    \n",
    "    def _load_annotations(self):\n",
    "        \"\"\"\n",
    "        Load all clip annotations.\n",
    "        Returns list of (video_path, landmarks_dir) tuples.\n",
    "        \"\"\"\n",
    "        annotations = []\n",
    "        for clip_dir in self.clips:\n",
    "            video_path = os.path.join(clip_dir, 'vid.avi')\n",
    "            landmarks_dir = os.path.join(clip_dir, 'annot')\n",
    "            \n",
    "            if not os.path.exists(video_path):\n",
    "                raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "            if not os.path.exists(landmarks_dir):\n",
    "                raise FileNotFoundError(f\"Landmarks directory not found: {landmarks_dir}\")\n",
    "                \n",
    "            # Get number of frames in video\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            cap.release()\n",
    "            \n",
    "            # Verify landmark files exist and have correct number of points\n",
    "            n_landmarks = None\n",
    "            for i in range(1, n_frames + 1):  # 300-VW uses 1-based indexing\n",
    "                pts_path = os.path.join(landmarks_dir, f'{str(i).zfill(6)}.pts')\n",
    "                if not os.path.exists(pts_path):\n",
    "                    raise FileNotFoundError(f\"Missing landmark file for frame {i} in {clip_dir}\")\n",
    "                \n",
    "                # Validate landmark count consistency\n",
    "                landmarks = self._load_pts_file(pts_path)\n",
    "                if n_landmarks is None:\n",
    "                    n_landmarks = len(landmarks)\n",
    "                elif len(landmarks) != n_landmarks:\n",
    "                    raise ValueError(f\"Inconsistent landmark count in {pts_path}. Expected {n_landmarks}, got {len(landmarks)}\")\n",
    "                    \n",
    "            self.n_landmarks = n_landmarks  # Store for reference\n",
    "            \n",
    "            annotations.append((video_path, landmarks_dir, n_frames))\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "    def _load_pts_file(self, pts_path):\n",
    "        landmarks = []\n",
    "        with open(pts_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            start_idx = 0\n",
    "            end_idx = len(lines)\n",
    "            \n",
    "            # Find the starting and ending brackets\n",
    "            for i, line in enumerate(lines):\n",
    "                if '{' in line:\n",
    "                    start_idx = i + 1\n",
    "                elif '}' in line:\n",
    "                    end_idx = i\n",
    "                    break\n",
    "            \n",
    "            # Parse coordinates\n",
    "            for line in lines[start_idx:end_idx]:\n",
    "                if line.strip():  # Skip empty lines\n",
    "                    try:\n",
    "                        x, y = map(float, line.strip().split())\n",
    "                        landmarks.append([x, y])\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error parsing line in {pts_path}: {line}\")\n",
    "                        raise e\n",
    "                        \n",
    "        return np.array(landmarks)\n",
    "    \n",
    "    def _load_video_segment(self, video_path, start_frame, n_frames):\n",
    "        \"\"\"\n",
    "        Load a segment of video frames efficiently.\n",
    "        \"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # Get video properties\n",
    "        orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        \n",
    "        # Calculate resize dimensions once\n",
    "        scale = self.crop_size / min(orig_h, orig_w)\n",
    "        new_h, new_w = int(orig_h * scale), int(orig_w * scale)\n",
    "        start_h = (new_h - self.crop_size) // 2\n",
    "        start_w = (new_w - self.crop_size) // 2\n",
    "        \n",
    "        # Set starting frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        \n",
    "        # Preallocate numpy array for batch processing\n",
    "        frames = np.empty((n_frames, self.crop_size, self.crop_size, 3), dtype=np.uint8)\n",
    "        \n",
    "        for i in range(n_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                raise ValueError(f\"Failed to read frame {start_frame + i} from {video_path}\")\n",
    "            \n",
    "            # Resize and crop\n",
    "            frame = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "            frame = frame[start_h:start_h + self.crop_size, \n",
    "                         start_w:start_w + self.crop_size]\n",
    "            frames[i] = frame\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Convert to tensor\n",
    "        return torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0  # (T, C, H, W)\n",
    "    \n",
    "    def _load_landmarks_segment(self, landmarks_dir, start_frame, n_frames):\n",
    "        \"\"\"\n",
    "        Load landmarks for a segment of frames.\n",
    "        \"\"\"\n",
    "        landmarks = []\n",
    "        for frame_idx in range(start_frame + 1, start_frame + n_frames + 1):\n",
    "            pts_path = os.path.join(landmarks_dir, f'{str(frame_idx).zfill(6)}.pts')\n",
    "            frame_landmarks = self._load_pts_file(pts_path)\n",
    "            \n",
    "            # Normalize landmarks to [0, 1] range\n",
    "            # Apply same transform as frames\n",
    "            h, w = frame_landmarks.max(axis=0) - frame_landmarks.min(axis=0)\n",
    "            scale = self.crop_size / min(h, w)\n",
    "            frame_landmarks = frame_landmarks * scale\n",
    "            \n",
    "            # Center crop adjustment\n",
    "            center = frame_landmarks.mean(axis=0)\n",
    "            crop_start = center - (self.crop_size / 2)\n",
    "            frame_landmarks = frame_landmarks - crop_start\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            frame_landmarks = frame_landmarks / self.crop_size\n",
    "            \n",
    "            # Clip to valid range\n",
    "            frame_landmarks = np.clip(frame_landmarks, 0, 1)\n",
    "            landmarks.append(frame_landmarks)\n",
    "            \n",
    "        return torch.tensor(landmarks).float()  # (T, N, 2)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a video clip and corresponding landmarks.\n",
    "        \"\"\"\n",
    "        video_path, landmarks_dir, total_frames = self.annotations[index]\n",
    "        \n",
    "        # Randomly sample a starting frame\n",
    "        max_start = total_frames - self.clip_len * self.frame_sample_rate\n",
    "        if max_start <= 0:\n",
    "            raise ValueError(f\"Video {video_path} is too short for the specified clip length and sampling rate\")\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            start_frame = np.random.randint(0, max_start)\n",
    "        else:\n",
    "            # For validation/test, take center clip\n",
    "            start_frame = max_start // 2\n",
    "            \n",
    "        # Load video frames and landmarks\n",
    "        frames = self._load_video_segment(\n",
    "            video_path, \n",
    "            start_frame, \n",
    "            self.clip_len * self.frame_sample_rate\n",
    "        )[::self.frame_sample_rate]  # Apply frame sampling\n",
    "        \n",
    "        landmarks = self._load_landmarks_segment(\n",
    "            landmarks_dir,\n",
    "            start_frame,\n",
    "            self.clip_len * self.frame_sample_rate\n",
    "        )[::self.frame_sample_rate]  # Apply frame sampling\n",
    "        \n",
    "        # Apply augmentations in training mode\n",
    "        if self.mode == 'train' and self.use_augmentation:\n",
    "            # Random horizontal flip\n",
    "            if torch.rand(1) > 0.5:\n",
    "                frames = torch.flip(frames, [-1])\n",
    "                landmarks[..., 0] = 1 - landmarks[..., 0]  # Flip x coordinates\n",
    "                \n",
    "                # TODO: Could add color augmentation here\n",
    "                # TODO: Could add random cropping here\n",
    "                \n",
    "        return frames, landmarks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(is_train, test_mode, args):\n",
    "    \"\"\"\n",
    "    Build 300-VW dataset.\n",
    "    \"\"\"\n",
    "    mode = 'train' if is_train else 'test' if test_mode else 'validation'\n",
    "    \n",
    "    dataset = VW300Dataset(\n",
    "        data_root=args.data_path,\n",
    "        mode=mode,\n",
    "        clip_len=args.num_frames,\n",
    "        frame_sample_rate=args.sampling_rate,\n",
    "        crop_size=args.input_size\n",
    "    )\n",
    "    \n",
    "    print(f\"300-VW dataset built successfully for mode: {mode}\")\n",
    "    print(f\"Number of clips: {len(dataset)}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def visualize_dataset(dataset, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize a few samples from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: An instance of VW300Dataset.\n",
    "        num_samples: Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    for idx in range(min(num_samples, len(dataset))):\n",
    "        frames, landmarks = dataset[idx]\n",
    "        \n",
    "        print(f\"Sample {idx + 1}:\")\n",
    "        print(f\"  Frames shape: {frames.shape}\")  # (T, C, H, W)\n",
    "        print(f\"  Landmarks shape: {landmarks.shape}\")  # (T, N, 2)\n",
    "        \n",
    "        # Visualize the first frame and its landmarks\n",
    "        frame = frames[0].permute(1, 2, 0).numpy()  # Convert to HWC format for visualization\n",
    "        landmark_points = landmarks[0].numpy()\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(frame)\n",
    "        plt.scatter(\n",
    "            landmark_points[:, 0] * frame.shape[1],  # Scale x coordinates to image size\n",
    "            landmark_points[:, 1] * frame.shape[0],  # Scale y coordinates to image size\n",
    "            color='red', s=10\n",
    "        )\n",
    "        plt.title(f\"Sample {idx + 1}: First Frame with Landmarks\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Build the dataset (adjust args accordingly)\n",
    "    class Args:\n",
    "        data_path = \"300vw_dataset\"\n",
    "        num_frames = 16\n",
    "        sampling_rate = 4\n",
    "        input_size = 224\n",
    "    \n",
    "    args = Args()\n",
    "    dataset = build_dataset(is_train=True, test_mode=False, args=args)\n",
    "    \n",
    "    # Visualize samples\n",
    "    visualize_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        # Dataset parameters\n",
    "        self.data_path = '300vw_dataset'\n",
    "        \n",
    "        # Model parameters\n",
    "        self.input_size = 224\n",
    "        self.num_frames = 16\n",
    "        self.sampling_rate = 4\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = 32\n",
    "        self.num_workers = 4\n",
    "        self.learning_rate = 0.001\n",
    "        self.log_interval = 10\n",
    "        self.epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialLandmarkCNN(nn.Module):\n",
    "    def __init__(self, args: Parameters, num_landmarks: int = 68):\n",
    "        super(FacialLandmarkCNN, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        # Load and modify ResNet backbone\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        # Remove final FC and pooling layers\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        \n",
    "        # Calculate output size from backbone\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, args.input_size, args.input_size)\n",
    "            backbone_output = self.backbone(dummy_input)\n",
    "            self.backbone_output_shape = backbone_output.shape[1:]\n",
    "        \n",
    "        # Temporal modeling with 3D convolutions\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv3d(self.backbone_output_shape[0], 128, kernel_size=(3, 3, 3), \n",
    "                     padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to ensure consistent size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((56, 56))\n",
    "        \n",
    "        # Refinement network with proper size handling\n",
    "        self.refinement = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Changed input channels from 512 to 256\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Landmark prediction head\n",
    "        self.landmark_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Changed input channels from 512 to 256\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_landmarks * 2, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights using Kaiming initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Conv3d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, num_frames, c, h, w = x.shape\n",
    "        \n",
    "        # Validate input dimensions\n",
    "        if num_frames != self.args.num_frames:\n",
    "            raise ValueError(f\"Expected {self.args.num_frames} frames, got {num_frames}\")\n",
    "        if h != self.args.input_size or w != self.args.input_size:\n",
    "            raise ValueError(f\"Expected size {self.args.input_size}, got {h}x{w}\")\n",
    "        \n",
    "        # Process all frames in parallel through backbone\n",
    "        x = x.view(batch_size * num_frames, c, h, w)\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Reshape features for temporal processing\n",
    "        _, c, h, w = features.shape\n",
    "        features = features.view(batch_size, num_frames, c, h, w)\n",
    "        features = features.permute(0, 2, 1, 3, 4)  # [B, C, T, H, W] format for 3D conv\n",
    "        \n",
    "        # Apply temporal convolution\n",
    "        features = self.temporal_conv(features)\n",
    "        \n",
    "        # Reshape back to 2D format\n",
    "        features = features.permute(0, 2, 1, 3, 4)  # [B, T, C, H, W]\n",
    "        features = features.contiguous().view(batch_size * num_frames, -1, h, w)\n",
    "        \n",
    "        # Apply adaptive pooling to ensure consistent size\n",
    "        features = self.adaptive_pool(features)\n",
    "        \n",
    "        # Extract landmarks and feature maps\n",
    "        landmarks = self.landmark_head(features)\n",
    "        landmarks = landmarks.view(batch_size, num_frames, -1, 2)\n",
    "        \n",
    "        # Generate feature maps\n",
    "        feature_maps = torch.sigmoid(self.refinement(features))\n",
    "        feature_maps = feature_maps.view(batch_size, num_frames, 1, h, w)\n",
    "        \n",
    "        return feature_maps, landmarks\n",
    "\n",
    "class FacialLandmarkDetector:\n",
    "    def __init__(self, args: Parameters):\n",
    "        self.args = args\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = FacialLandmarkCNN(args).to(self.device)\n",
    "        \n",
    "        # Initialize loss functions with reduction method\n",
    "        self.criterion_landmarks = nn.MSELoss(reduction='mean')\n",
    "        self.criterion_features = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        \n",
    "        # Initialize optimizer with weight decay\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.args.learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Initialize learning rate scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=5,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Initialize loss history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def create_landmark_map(self, landmarks: torch.Tensor, height: int, \n",
    "                          width: int, sigma: float = 3.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create Gaussian heatmaps for landmarks with numerical stability improvements.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: Tensor of shape (batch_size, num_landmarks, 2)\n",
    "            height: Height of output heatmap\n",
    "            width: Width of output heatmap\n",
    "            sigma: Standard deviation for Gaussian kernel\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, 1, height, width)\n",
    "        \"\"\"\n",
    "        batch_size = landmarks.size(0)\n",
    "        landmark_map = torch.zeros(batch_size, 1, height, width, \n",
    "                                 device=self.device)\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        y_grid, x_grid = torch.meshgrid(\n",
    "            torch.arange(height, device=self.device),\n",
    "            torch.arange(width, device=self.device)\n",
    "        )\n",
    "        \n",
    "        # Pre-compute maximum square distance for clipping\n",
    "        max_squared_dist = 9 * sigma * sigma\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for landmark in landmarks[b]:\n",
    "                x, y = landmark\n",
    "                \n",
    "                # Skip if landmark is outside image bounds\n",
    "                if not (0 <= x < width and 0 <= y < height):\n",
    "                    continue\n",
    "                \n",
    "                # Compute distances efficiently using broadcasting\n",
    "                squared_dist = (x_grid - x) ** 2 + (y_grid - y) ** 2\n",
    "                \n",
    "                # Clip distances for numerical stability\n",
    "                squared_dist = torch.clamp(squared_dist, max=max_squared_dist)\n",
    "                \n",
    "                # Compute Gaussian values\n",
    "                gaussian = torch.exp(-squared_dist / (2 * sigma * sigma))\n",
    "                \n",
    "                # Update landmark map using maximum values\n",
    "                landmark_map[b, 0] = torch.maximum(landmark_map[b, 0], gaussian)\n",
    "        \n",
    "        return landmark_map\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        \"\"\"Train for one epoch and return average loss.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        for batch_idx, (data, target_landmarks) in enumerate(train_loader):\n",
    "            # Skip invalid batches\n",
    "            if data.size(1) != self.args.num_frames:\n",
    "                continue\n",
    "            \n",
    "            # Move data to device\n",
    "            data = data.to(self.device)\n",
    "            target_landmarks = target_landmarks.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            feature_maps, predicted_landmarks = self.model(data)\n",
    "            \n",
    "            # Create target maps efficiently\n",
    "            target_maps = self.create_landmark_map(\n",
    "                target_landmarks.view(-1, target_landmarks.size(-2), 2),\n",
    "                data.size(-2), data.size(-1)\n",
    "            ).view_as(feature_maps)\n",
    "            \n",
    "            # Compute losses\n",
    "            landmark_loss = self.criterion_landmarks(\n",
    "                predicted_landmarks, target_landmarks\n",
    "            )\n",
    "            feature_loss = self.criterion_features(feature_maps, target_maps)\n",
    "            loss = landmark_loss + feature_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Log progress\n",
    "            if batch_idx % self.args.log_interval == 0:\n",
    "                logger.info(\n",
    "                    f'Train Batch: {batch_idx}/{num_batches} '\n",
    "                    f'Loss: {loss.item():.6f}'\n",
    "                )\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        \"\"\"Validate the model and return average validation loss.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(val_loader)\n",
    "        \n",
    "        for data, target_landmarks in val_loader:\n",
    "            if data.size(1) != self.args.num_frames:\n",
    "                continue\n",
    "            \n",
    "            data = data.to(self.device)\n",
    "            target_landmarks = target_landmarks.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            feature_maps, predicted_landmarks = self.model(data)\n",
    "            \n",
    "            # Create target maps\n",
    "            target_maps = self.create_landmark_map(\n",
    "                target_landmarks.view(-1, target_landmarks.size(-2), 2),\n",
    "                data.size(-2), data.size(-1)\n",
    "            ).view_as(feature_maps)\n",
    "            \n",
    "            # Compute losses\n",
    "            landmark_loss = self.criterion_landmarks(\n",
    "                predicted_landmarks, target_landmarks\n",
    "            )\n",
    "            feature_loss = self.criterion_features(feature_maps, target_maps)\n",
    "            loss = landmark_loss + feature_loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader, \n",
    "              num_epochs: Optional[int] = None) -> None:\n",
    "        \"\"\"Train the model for specified number of epochs.\"\"\"\n",
    "        num_epochs = num_epochs or self.args.epochs\n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            logger.info(f'\\nEpoch: {epoch+1}/{num_epochs}')\n",
    "            \n",
    "            # Train and validate\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.validate(val_loader)\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Store losses\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            logger.info(f'Training Loss: {train_loss:.6f}')\n",
    "            logger.info(f'Validation Loss: {val_loss:.6f}')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                \n",
    "                # Save checkpoint\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'train_losses': self.train_losses,\n",
    "                    'val_losses': self.val_losses\n",
    "                }, 'best_model.pth')\n",
    "                \n",
    "                logger.info('Saved best model checkpoint')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                logger.info(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "                break\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, video_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract features and landmarks from a video tensor.\"\"\"\n",
    "        self.model.eval()\n",
    "        video_tensor = video_tensor.to(self.device)\n",
    "        \n",
    "        if video_tensor.size(1) != self.args.num_frames:\n",
    "            raise ValueError(\n",
    "                f\"Expected {self.args.num_frames} frames, \"\n",
    "                f\"got {video_tensor.size(1)}\"\n",
    "            )\n",
    "        \n",
    "        feature_maps, landmarks = self.model(video_tensor)\n",
    "        return feature_maps, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize parameters\n",
    "    args = Parameters()\n",
    "    logger.info(\"Initialized parameters\")\n",
    "    \n",
    "    # Create detector instance\n",
    "    detector = FacialLandmarkDetector(args)\n",
    "    logger.info(\"Created facial landmark detector\")\n",
    "    \n",
    "    try:\n",
    "        train_dataset = build_dataset(is_train=True, test_mode=False, args=args)\n",
    "        val_dataset = build_dataset(is_train=False, test_mode=False, args=args)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=args.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers, \n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=args.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=args.num_workers, \n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Training configuration\n",
    "        logger.info(\"Starting training...\")\n",
    "        detector.train(train_loader, val_loader)\n",
    "        logger.info(\"Training completed successfully\")\n",
    "        \n",
    "        # Save final model\n",
    "        torch.save({\n",
    "            'model_state_dict': detector.model.state_dict(),\n",
    "            'optimizer_state_dict': detector.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': detector.scheduler.state_dict(),\n",
    "            'train_losses': detector.train_losses,\n",
    "            'val_losses': detector.val_losses\n",
    "        }, 'final_model.pth')\n",
    "        logger.info(\"Saved final model\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during training: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return detector\n",
    "\n",
    "def load_pretrained_detector(checkpoint_path: str, args: Parameters = None) -> FacialLandmarkDetector:\n",
    "    \"\"\"\n",
    "    Load a pretrained facial landmark detector from a checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "        args: Optional Parameters object. If None, default parameters will be used.\n",
    "        \n",
    "    Returns:\n",
    "        FacialLandmarkDetector: Loaded detector with pretrained weights\n",
    "    \"\"\"\n",
    "    if args is None:\n",
    "        args = Parameters()\n",
    "    \n",
    "    try:\n",
    "        # Create detector instance\n",
    "        detector = FacialLandmarkDetector(args)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=detector.device)\n",
    "        \n",
    "        # Load model state\n",
    "        detector.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Optionally load optimizer and scheduler states if they exist\n",
    "        if 'optimizer_state_dict' in checkpoint:\n",
    "            detector.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            detector.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        # Load training history if available\n",
    "        if 'train_losses' in checkpoint:\n",
    "            detector.train_losses = checkpoint['train_losses']\n",
    "        if 'val_losses' in checkpoint:\n",
    "            detector.val_losses = checkpoint['val_losses']\n",
    "        \n",
    "        logger.info(f\"Successfully loaded pretrained model from {checkpoint_path}\")\n",
    "        return detector\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading pretrained model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        detector = main()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Program terminated with error: {str(e)}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
